{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Youtube/GDP](Top)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/ACFFpv2.png width=1500 class=\"center\">\n",
    "<h1 align=\"center\">Top Youtubers effecting GDP between 2019 - 2022??</h1>\n",
    "    \n",
    "    In today's world, the internet has become an integral part of our lives. With the rise of online platforms such as YouTube, it has become easier than ever for people to access information and entertainment from all over the world. At the same time, Gross Domestic Product (GDP) remains one of the most widely used indicators of economic performance. In this project, we aim to explore the relationship between these two seemingly unrelated topics: GDP and YouTube. By analyzing data on GDP and YouTube usage patterns, we hope to gain insights into how these two factors are connected and what implications this may have on our understanding of the modern world.\n",
    "\n",
    "The objective of this project is to analyze the relationship between GDP and YouTube usage patterns. We will use data on GDP and YouTube usage patterns to gain insights into how these two factors are connected and what implications this may have on our understanding of the modern world.\n",
    "\n",
    "Our goal is to answer the following questions:\n",
    "\n",
    "- What are the the top Youtubers in countries around the globe?\n",
    "- What is the Top GDP countries, and what is there growth during COVID?\n",
    "- Is there a correlation between Top Youtubers and selected GDP Nations?\n",
    "- What is statistical corelations can be made?\n",
    "\n",
    "To answer these questions, we will use Python and its data analysis libraries, such as Pandas and Matplotlib. We will start by importing the dataset and cleaning the data, followed by exploratory data analysis and visualization.\n",
    "\n",
    "I will be using the following datasets: \n",
    "- [Top Youtubers](https://www.kaggle.com/mdhrumil/top-5000-youtube-channels-data-from-socialblade)\n",
    "- [GDP](https://www.kaggle.com/fernandol/countries-of-the-world)\n",
    "\n",
    "APIs:\n",
    "- [YouTube API](https://developers.google.com/youtube/v3/docs/channels/list)\n",
    "- [Google API](https://console.cloud.google.com/apis/library/youtube.googleapis.com)\n",
    "\n",
    "\n",
    "\n",
    "# Table of contents <a class='anchor' id='top'>\n",
    "- [Introduction](#Introduction)\n",
    "- [Import libraries](#import)\n",
    "- [Load data](#load_data)\n",
    "- [GDP Analysis](#gdpproject)\n",
    "- [Bar chart](#bar_chart)\n",
    "- [GDP Conclusion](#geo)\n",
    "- [YouTube Analysis](#Analysis)\n",
    "- [Youtube API](#YouTube)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  <a class='anchor' id='Introduction'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries <a class='anchor' id='import'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: isodate in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.6.1)\n",
      "Requirement already satisfied: six in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from isodate) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-auth in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.106.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-oauthlib) (1.3.1)\n",
      "Requirement already satisfied: httplib2>=0.19.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-httplib2) (0.22.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client) (2.12.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.61.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install isodate\n",
    "%pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "%pip install wordcloud\n",
    "%pip install nltk\n",
    "%pip install wbgapi\n",
    "%pip install bar_chart_race\n",
    "%pip install plotly\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install numpy\n",
    "%pip install requests\n",
    "%pip install scipy\n",
    "%pip install sklearn\n",
    "%pip install statsmodels\n",
    "%pip install xgboost\n",
    "%pip install lightgbm\n",
    "%pip install catboost\n",
    "%pip install bar_chart_race\n",
    "%pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jamal\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "\n",
    "# Data visualization libraries\n",
    "%pip install scikit-learn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pyplot as pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Google API\n",
    "from googleapiclient.discovery import build\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jamal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jamal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLP libraries\n",
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <b><u><span style=\"font-size: 24px\">\n",
    "  GDP Analysis from 2019 - 2020<a class='anchor' id='gdpproject'></span></u></b><br>\n",
    "</p>\n",
    "\n",
    "### Load data <a class='anchor' id='load_data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#load data\n",
    "data = pd.read_csv('GDP by Country 1999-2022.csv', decimal = ',')\n",
    "\n",
    "data = data.replace({'\\\"' :''}, regex=True)\n",
    "data = data.replace({',' :''}, regex=True)\n",
    "#display(data)\n",
    "\n",
    "data = data.astype({'1999' : 'float', '2022' : 'float'})\n",
    "\n",
    "#observe data\n",
    "data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart for Top 10 GDP countries and for targeted group <a class='anchor' id='bar_chart'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 GDP in 2022\n",
    "top_10_GDP_2022 = data.sort_values('2022', ascending = False).head(10)\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "sns.barplot(x = 'Country', y = '2022', data = top_10_GDP_2022, palette = 'Set1')\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Gross Domestic Product', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.xticks(rotation = 90)\n",
    "pyplot.title('Top GDP producing Countries in 2022', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022 GDP for germany, south korea, UK, US, Mexico, japan and india\n",
    "\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'United States', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries.sort_values('2022', ascending = False)\n",
    "\n",
    "display(Countries_GDP_2022)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "sns.barplot(x = 'Country', y = '2022', data = Countries_GDP_2022, legend=False, palette = 'Set1')\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('GDP by Country 2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.xticks(rotation = 45)\n",
    "pyplot.title('2022 GDP for Germany, South korea, UK, US, Mexico, Japan and India', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Gross Domestic product average for 2019 - 2020 <a class='anchor' id='19-20'>\n",
    "+ Germany, South Korea UK, US, Mexico, Japan and India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average GDP between 2018-2022 for germany, south korea, UK, US, Mexico, japan and india\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'United States', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries[['Country', '2018', '2019', '2020', '2021', '2022']]\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "new_data = pd.melt(Countries_GDP_2022, id_vars = ['Country'], value_vars = ['2018', '2019', '2020', '2021', '2022'], var_name = 'Year', value_name = 'GDP')\n",
    "\n",
    "new_data = new_data.astype({'Country' : 'str'})\n",
    "new_data = new_data.astype({'GDP' : 'float'})\n",
    "\n",
    "\n",
    "display(new_data)\n",
    "\n",
    "sns.pointplot(data = new_data, x = 'Year', y = 'GDP', hue = 'Country', ax = ax, palette = 'nipy_spectral')\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Average GDP by Country 2018-2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.title('Average GDP between 2018-2022', fontsize = 20)\n",
    "pyplot.show()\n",
    "\n",
    "#sort_new_data = new_data.sort_values('GDP', ascending = True)\n",
    "#sort_new_data = resort_new_data.sort_values('GDP', ascending = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sort_new_data = new_data.sort_values('GDP', ascending = True)\n",
    "#sort_new_data = resort_new_data.sort_values('GDP', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average GDP between 2018-2022 for germany, south korea, UK, Mexico, japan and india ***Without the US because it made the graph look bad***\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries[['Country', '2018', '2019', '2020', '2021', '2022']]\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 8))\n",
    "new_data = pd.melt(Countries_GDP_2022, id_vars = ['Country'], value_vars = ['2018', '2019', '2020', '2021', '2022'], var_name = 'Year', value_name = 'GDP')\n",
    "\n",
    "new_data = new_data.astype({'Country' : 'str'})\n",
    "new_data = new_data.astype({'GDP' : 'float'})\n",
    "\n",
    "\n",
    "\n",
    "display(new_data)\n",
    "\n",
    "sns.pointplot(data = new_data, x = 'Year', y = 'GDP', hue = 'Country', ax = ax, palette = 'nipy_spectral')\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Average GDP by Country 2018-2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.title('Average GDP between 2018-2022 without the U.S.', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HalfTime <a class='anchor' id='geo'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Region Code: You must provide a two-letter ISO 3166–1 country code \n",
    "* (e.g., ‘US’ for the United States) to specify the region for which you want to find the top 10 YouTube channels.\n",
    "* Replace 'YOUR_REGION_CODE' in the code with the desired region code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ISO CODE ALPHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GDP by Country 1999-2022.csv\", sep=',', header=0, thousands=\",\")\n",
    "df.set_index(\"Country\", inplace=True)\n",
    "df = df.T\n",
    "print(f\"Dataframe has {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, display\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Exploratory Data Analysis <a class='anchor' id='Analysis'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube API <a class='anchor' id='YouTube'></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data creation with Youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk'  # Youtube API key Personal \n",
    "\n",
    "channel_ids = ['UCX6OQ3DkcsbYNE6H8uQQuVA', # Mr. Beast US\n",
    "               'UCbCmjCuTUZos6Inko4u57UQ', # Cocomelon US\n",
    "               'UCqwUrj10mAEsqezcItqvwEw', # Bhuvaneshwar Bam IN\n",
    "               'UC_vcKmg67vjMP7ciLnSxSHQ', # Amit Bhadana IN\n",
    "               'UCjp_3PEaOau_nT_3vnqKIvg', # HikakinTV JP\n",
    "               'UC-lHJZR3Gqxm24_Vd_AJ5Yw', # PewDiePie SE\n",
    "               'UCYWOjHweP2V-8kGKmmAmQJQ', # Badabun MX \n",
    "               'UCECJDeK0MNapZbpaOzxrUPA', # Luisito Comunica MX\n",
    "               'UCOmHUn--16B90oW2L6FRR3A', # BLACKPINK\n",
    "              ]\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to pull data from Youtube API and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New method to pull data from Youtube API and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  channelName subscribers         views totalVideos  \\\n",
      "0                 BB Ki Vines    26300000    4825681963         190   \n",
      "1            Luisito Comunica    41400000    9053087521        1316   \n",
      "2  Cocomelon - Nursery Rhymes   167000000  170365245506        1028   \n",
      "3                Amit Bhadana    24500000    2429601444         106   \n",
      "4                  Junya.じゅんや    30800000   18789977842        7264   \n",
      "\n",
      "                 playlistId  \n",
      "0  UUqwUrj10mAEsqezcItqvwEw  \n",
      "1  UUECJDeK0MNapZbpaOzxrUPA  \n",
      "2  UUbCmjCuTUZos6Inko4u57UQ  \n",
      "3  UU_vcKmg67vjMP7ciLnSxSHQ  \n",
      "4  UUjp_3PEaOau_nT_3vnqKIvg  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='youtube_data.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize YouTube API client\n",
    "# Replace 'YOUR_API_KEY' with your actual API key\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk')\n",
    "\n",
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "def get_top_video_ids_by_view_count(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    #Get the top 10 video IDs from the given playlist sorted by view count!\n",
    "    # Fetch video IDs from the playlist\n",
    "    # ...\n",
    "    # Sort video IDs by view count and return top 10\n",
    "    # ...\n",
    "    Params:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    playlist_id (str): Playlist ID.\n",
    "    \n",
    "    Returns:\n",
    "    List of the top 10 video IDs in the playlist sorted by view count.\n",
    "    \"\"\"\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract video IDs and view counts into a list of dictionaries\n",
    "    videos_data = [{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])]\n",
    "\n",
    "    # If there are more pages, continue fetching\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token and len(videos_data) < 10:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part='contentDetails',\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        videos_data.extend([{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # Sort the videos by view count in descending order and return the top 10\n",
    "    top_video_ids = [video['video_id'] for video in videos_data]\n",
    "    return top_video_ids[:10]\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get details for the provided video IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video details.\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                id=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            # Process response data and append to all_video_info\n",
    "            video_info = response['items'][0]\n",
    "            snippet = video_info['snippet']\n",
    "            statistics = video_info['statistics']\n",
    "            content_details = video_info['contentDetails']\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'channelTitle': snippet.get('channelTitle', ''),\n",
    "                'title': snippet.get('title', ''),\n",
    "                'description': snippet.get('description', ''),\n",
    "                'tags': snippet.get('tags', []),\n",
    "                'publishedAt': snippet.get('publishedAt', ''),\n",
    "                'viewCount': statistics.get('viewCount', 0),\n",
    "                'likeCount': statistics.get('likeCount', 0),\n",
    "                'favoriteCount': statistics.get('favoriteCount', 0),\n",
    "                'commentCount': statistics.get('commentCount', 0),\n",
    "                'duration': content_details.get('duration', ''),\n",
    "                'definition': content_details.get('definition', ''),\n",
    "                'caption': content_details.get('caption', ''),\n",
    "            }\n",
    "            \n",
    "            all_video_info.append(video_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching details for video {video_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "def get_top_liked_comments(youtube, video_ids, max_results=10, min_likes=10):\n",
    "    \"\"\"\n",
    "    Get the top liked comments for each provided video ID and filter out spam-like comments.\n",
    "\n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "    max_results (int): Maximum number of comments to retrieve for each video (default is 10).\n",
    "    min_likes (int): Minimum number of likes for a comment to be considered non-spam (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video IDs and associated top liked comments.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,  # Fetch the specified number of comments\n",
    "                order=\"relevance\",  # Order comments by relevance (likely to include top liked comments)\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Process comments and filter out spam-like comments\n",
    "            comments_in_video = []\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                like_count = snippet.get('likeCount', 0)\n",
    "                comment_text = snippet.get('textDisplay', '')\n",
    "\n",
    "                # Filter out spam-like comments based on the minimum likes threshold\n",
    "                if like_count >= min_likes:\n",
    "                    comments_in_video.append(comment_text)\n",
    "\n",
    "            comments_in_video_info = {'video_id': video_id,'comments': comments_in_video}\n",
    "            all_comments.append(comments_in_video_info)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids, max_results=10, min_likes=50):\n",
    "    \"\"\"\n",
    "    # ... (Implementation of get_top_liked_comments function)\n",
    "    Get the top 10 comments for each provided video ID.\n",
    "\n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video IDs and associated top-level comments in text.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,  # Fetch only the first 10 comments #UPDATED to Fetch the specified number of comments\n",
    "                order=\"relevance\",  # Order comments by relevance (likely to include top liked comments) \n",
    "            )\n",
    "            response = request.execute()\n",
    "            # Process response data and append to all_comments # UPDATED to Process comments and filter out spam-like comments\n",
    "            # Process comments and filter out spam-like comments\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'comment_id': item['id'],\n",
    "                    'author': snippet.get('authorDisplayName', ''),\n",
    "                    'text': snippet.get('textDisplay', ''),\n",
    "                    'like_count': snippet.get('likeCount', 0),\n",
    "                    'published_at': snippet.get('publishedAt', ''),\n",
    "                }\n",
    "                all_comments.append(comment_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # List of channel IDs to process\n",
    "    channel_ids = ['UCX6OQ3DkcsbYNE6H8uQQuVA','UCbCmjCuTUZos6Inko4u57UQ','UCqwUrj10mAEsqezcItqvwEw','UC_vcKmg67vjMP7ciLnSxSHQ','UCjp_3PEaOau_nT_3vnqKIvg','UC-lHJZR3Gqxm24_Vd_AJ5Yw', 'UCYWOjHweP2V-8kGKmmAmQJQ','UCECJDeK0MNapZbpaOzxrUPA','UCOmHUn--16B90oW2L6FRR3A']\n",
    "    # Get channel statistics\n",
    "    channel_data = get_channel_stats(youtube, channel_ids)\n",
    "    \n",
    "    # Print the first few rows of channel_data to inspect column names\n",
    "    print(channel_data.head())\n",
    "    \n",
    "    video_df = pd.DataFrame()\n",
    "    comments_df = pd.DataFrame()\n",
    "    \n",
    "    # Ensure the column name matches the actual column name in channel_data\n",
    "    for channel in channel_data['channelName'].unique():\n",
    "        logging.info(f\"Processing channel: {channel}\")\n",
    "        playlist_id = channel_data.loc[channel_data['channelName'] == channel, 'playlistId'].iloc[0]\n",
    "        try:\n",
    "            video_ids = get_top_video_ids_by_view_count(youtube, playlist_id)\n",
    "            video_data = get_video_details(youtube, video_ids)\n",
    "            comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "            video_df = pd.concat([video_df, video_data]).drop_duplicates(subset=['video_id'])\n",
    "            comments_df = pd.concat([comments_df, comments_data]).drop_duplicates(subset=['video_id'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing channel {channel}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Save the data to CSV files\n",
    "    video_df.to_csv('video_data.csv', index=False)\n",
    "    comments_df.to_csv('comments_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get channel statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_channel_stats` function defined below, now we are going to obtain the channel statistics for the 9 channels in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist, and country.\n",
    "    \n",
    "    Parameters:\n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist, and country.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for channel_id in channel_ids:\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=channel_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Fetch additional data including country\n",
    "            snippet_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            snippet_response = snippet_request.execute()\n",
    "            country = snippet_response['items'][0]['snippet'].get('country', '')\n",
    "\n",
    "            data = dict(\n",
    "                channelName=response['items'][0]['snippet']['title'],\n",
    "                subscribers=response['items'][0]['statistics']['subscriberCount'],\n",
    "                views=response['items'][0]['statistics']['viewCount'],\n",
    "                totalVideos=response['items'][0]['statistics']['videoCount'],\n",
    "                playlistId=response['items'][0]['contentDetails']['relatedPlaylists']['uploads'],\n",
    "                country=country  # Add country information\n",
    "            )\n",
    "            all_data.append(data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching channel stats for {channel_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelName</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>views</th>\n",
       "      <th>totalVideos</th>\n",
       "      <th>playlistId</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MrBeast</td>\n",
       "      <td>208000000</td>\n",
       "      <td>36332259733</td>\n",
       "      <td>765</td>\n",
       "      <td>UUX6OQ3DkcsbYNE6H8uQQuVA</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>167000000</td>\n",
       "      <td>170365245506</td>\n",
       "      <td>1028</td>\n",
       "      <td>UUbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BB Ki Vines</td>\n",
       "      <td>26300000</td>\n",
       "      <td>4825681963</td>\n",
       "      <td>190</td>\n",
       "      <td>UUqwUrj10mAEsqezcItqvwEw</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amit Bhadana</td>\n",
       "      <td>24500000</td>\n",
       "      <td>2429601444</td>\n",
       "      <td>106</td>\n",
       "      <td>UU_vcKmg67vjMP7ciLnSxSHQ</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junya.じゅんや</td>\n",
       "      <td>30800000</td>\n",
       "      <td>18789977842</td>\n",
       "      <td>7264</td>\n",
       "      <td>UUjp_3PEaOau_nT_3vnqKIvg</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channelName subscribers         views totalVideos  \\\n",
       "0                     MrBeast   208000000   36332259733         765   \n",
       "1  Cocomelon - Nursery Rhymes   167000000  170365245506        1028   \n",
       "2                 BB Ki Vines    26300000    4825681963         190   \n",
       "3                Amit Bhadana    24500000    2429601444         106   \n",
       "4                  Junya.じゅんや    30800000   18789977842        7264   \n",
       "\n",
       "                 playlistId country  \n",
       "0  UUX6OQ3DkcsbYNE6H8uQQuVA      US  \n",
       "1  UUbCmjCuTUZos6Inko4u57UQ      US  \n",
       "2  UUqwUrj10mAEsqezcItqvwEw      IN  \n",
       "3  UU_vcKmg67vjMP7ciLnSxSHQ      IN  \n",
       "4  UUjp_3PEaOau_nT_3vnqKIvg      JP  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "    \n",
    "for channel_id in channel_ids:\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,contentDetails,statistics',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Fetch additional data including country\n",
    "        snippet_request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            id=channel_id\n",
    "        )\n",
    "        snippet_response = snippet_request.execute()\n",
    "        country = snippet_response['items'][0]['snippet'].get('country', '')\n",
    "\n",
    "        data = dict(\n",
    "            channelName=response['items'][0]['snippet']['title'],\n",
    "            subscribers=response['items'][0]['statistics']['subscriberCount'],\n",
    "            views=response['items'][0]['statistics']['viewCount'],\n",
    "            totalVideos=response['items'][0]['statistics']['videoCount'],\n",
    "            playlistId=response['items'][0]['contentDetails']['relatedPlaylists']['uploads'],\n",
    "            country=country  # Add country information\n",
    "        )\n",
    "        all_data.append(data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching channel stats for {channel_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "channel_data1 = pd.DataFrame(all_data)\n",
    "channel_data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert count columns to numeric columns\n",
    "numeric_cols = ['subscribers', 'views', 'totalVideos']\n",
    "channel_data1[numeric_cols] = channel_data1[numeric_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count columns to numeric columns\n",
    "numeric_cols = ['subscribers', 'views', 'totalVideos']\n",
    "channel_data1[numeric_cols] = channel_data1[numeric_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data1.to_csv('Topchannel_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data.to_csv('channel_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Viewership ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Using Channel_data dataframe from previous step, the data is sorted, and the top 9 channels are plotted with a legend for clarity.\n",
    "\n",
    "# Sort the data and get the top 9 channels for clarity in the legend\n",
    "sorted_data = channel_data.sort_values('subscribers', ascending=False)[0:9]\n",
    "\n",
    "# Create a color palette with a distinct color for each bar\n",
    "palette = sns.color_palette(\"hsv\", len(sorted_data))\n",
    "\n",
    "# Create the bar plot with the specified palette\n",
    "ax = sns.barplot(\n",
    "    x='channelName', \n",
    "    y='subscribers', \n",
    "    data=sorted_data, \n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format the y-axis labels as thousands with a K suffix\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000:.0f}K'))\n",
    "\n",
    "# Remove x-axis labels as we will add these as legends\n",
    "ax.set(xticklabels=[])\n",
    "\n",
    "# Create the legend manually\n",
    "for i, row in enumerate(sorted_data.itertuples()):\n",
    "    plt.bar(0, 0, color=palette[i], label=row.channelName,)\n",
    "\n",
    "# Place the legend below the chart # Adjust legend box height and width\n",
    "plt.legend(title='The Top YouTuber Channel Names', bbox_to_anchor=(1.35, .5), loc='center')\n",
    "\n",
    "# Add labels to the bars to show the exact subscriber count for each channel\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * height, \n",
    "            f'{height/1000:.0f}K', ha='center', va='bottom', fontsize=8, color='black'\n",
    "            )\n",
    "\n",
    "#  Add a title and format it\n",
    "plt.title('Top 9 YouTuber Channels by Subscriber Count', x=0.7, y=1.15, loc='Center', fontsize=18, fontweight='bold')\n",
    "\n",
    "\n",
    "# FILEPATH: /c:/Users/jamal/jamaleb67.github.io/jamaleb67.github.io/work/Project.ipynb\n",
    "plt.suptitle('Source: YouTube API', x=0.05, y=0.02, ha='left', fontsize=7)\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"TOP9PLOT.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "ax = sns.barplot(x='channelName', y='views', data=channel_data.sort_values('views', ascending=False))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "\n",
    "\n",
    "# Using Channel_data dataframe from previous step, the data is sorted, and the top 9 channels are plotted with a legend for clarity.\n",
    "\n",
    "# Sort the data and get the top 9 channels for clarity in the legend\n",
    "sorted_data = channel_data.sort_values('views', ascending=False)[0:9]\n",
    "\n",
    "# Create a color palette with a distinct color for each bar\n",
    "palette = sns.color_palette(\"hsv\", len(sorted_data))\n",
    "\n",
    "# Create the bar plot with the specified palette\n",
    "ax = sns.barplot(\n",
    "    x='channelName', \n",
    "    y='views', \n",
    "    data=sorted_data, \n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format the y-axis labels as thousands with a K suffix\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000:.0f}K'))\n",
    "\n",
    "# Remove x-axis labels as we will add these as legends\n",
    "ax.set(xticklabels=[])\n",
    "\n",
    "# Create the legend manually\n",
    "for i, row in enumerate(sorted_data.itertuples()):\n",
    "    plt.bar(0, 0, color=palette[i], label=row.channelName,)\n",
    "\n",
    "# Place the legend below the chart # Adjust legend box height and width\n",
    "plt.legend(title='The Top YouTuber Channels by Viewership', bbox_to_anchor=(1.35, .5), loc='center')\n",
    "\n",
    "# Add labels to the bars to show the exact subscriber count for each channel\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * height, \n",
    "            f'{height/1000:.0f}K', ha='center', va='bottom', rotation=10,  fontsize=7\n",
    "            )\n",
    "\n",
    "#  Add a title and format it\n",
    "plt.title('Top 9 YouTuber Channels by View Count', x=0.7, y=1.15, loc='Center', fontsize=18, fontweight='bold')\n",
    "\n",
    "\n",
    "# FILEPATH: /c:/Users/jamal/jamaleb67.github.io/jamaleb67.github.io/work/Project.ipynb\n",
    "plt.suptitle('Source: YouTube API', x=0.05, y=0.02, ha='left', fontsize=8)\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"TOP9PLOT.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Video Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video statistics are obtained using the `get_video_stats` function defined below. The function takes in a list of video ids and returns a dataframe with the video statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(video_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "\n",
    "#video_df = pd.DataFrame()\n",
    "#comments_df = pd.DataFrame() \n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    # get comment data\n",
    "    comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    video_df = video_df.concat(video_data, ignore_index=True)\n",
    "    comments_df = comments_df.concat(comments_data, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "import pandas as pd\n",
    "\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    video_df = video_df.append(video_data, ignore_index=True)\n",
    "    comments_df = comments_df.concat(comments_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "video_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName'] == c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "\n",
    "    # append video data together\n",
    "    video_df = pd.concat([video_df, video_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write video data to CSV file for future references\n",
    "video_df.to_csv('video_data_top10_channels.csv')\n",
    "comments_df.to_csv('comments_data_top10_channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed a Power BI report in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a class='anchor' id='Conclusion'></a>[↑](#Top)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "#### Get all items\n",
    "\n",
    "```http\n",
    "  GET /api/items\n",
    "```\n",
    "\n",
    "| Parameter | Type     | Description                |\n",
    "| :-------- | :------- | :------------------------- |\n",
    "| `api_key` | `string` | **AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk**. YouTube API key |\n",
    "\n",
    "#### Get item\n",
    "\n",
    "```http\n",
    "  GET /api/items/${id}\n",
    "```\n",
    "\n",
    "| Parameter | Type     | Description                       |\n",
    "| :-------- | :------- | :-------------------------------- |\n",
    "| `id`      | `string` | **UCtYLUTtgS3k1Fg4y5tAhLbw** # Statquest\n",
    "| `id`      | `string` | 'UCCezIgC97PvUuR4_gbFUs5g', # Corey Schafer\n",
    "| `id`      | `string` | 'UCfzlCWGWYyIQ0aLC5w48gBQ', # Sentdex\n",
    "| `id`      | `string` | 'UCNU_lfiiWBdtULKOw6X0Dig', # Krish Naik\n",
    "| `id`      | `string` | 'UCzL_0nIe8B4-7ShhVPfJkgw', # DatascienceDoJo\n",
    "| `id`      | `string` | 'UCLLw7jmFsvfIVaUFsLs8mlQ', # Luke Barousse \n",
    "| `id`      | `string` | 'UCiT9RITQ9PW6BhXK0y2jaeg', # Ken Jee\n",
    "| `id`      | `string` | 'UC7cs8q-gJRlGwj4A8OmCmXg', # Alex the analyst\n",
    "| `id`      | `string` | 'UC2UXDak6o7rBm23k3Vv5dww', # Tina Huang\n",
    "\n",
    "\n",
    "#### add(more to follow)\n",
    "\n",
    "Takes two numbers and returns the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badges\n",
    "\n",
    "\n",
    "[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link to top of page <a class='anchor' id='Top'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
